\chapter{Estrategia general y tratamiento estadístico}
\label{ch:strategy}
\epigraph{\emph{``There is a crack in everything. That's how the light gets in.”}}{Leonard Cohen}





El análisis realizado para esta tesis consiste en búsquedas de nuevas partículas predichas por modelos \ac{BSM} con un fotón aislado y altamente energético en asociación con jets~\cite{ATLAS-PhotonJetResonances-Run2-NOTE}. La estrategia consiste en la búsqueda de excesos locales de eventos producidos por nuevas resonancias predichas por modelos exóticos \ac{BSM} (de ahora en más llamados bumps) en la masa invariante del estado final de un par de fotón+jet. Este exceso sobre las predicciones de este fondo del \ac{SM} presenta una forma de decaimiento suave, conduciendo a un escenario ideal para la aplicación de la estrategia mencionada.

La investigación presentada en esta tesis está motivada, por un lado, por diferentes modelos teóricos de \acf{EQ} y de \acf{QBH} introducidos en la \Sect{\ref{sec:theory:bsm}}, y por otro, por una búsqueda genérica de resonancias de tipo gaussianas con diferentes valores de su media y desviación estándar. Este modelo proporciona una interpretación genérica para la presencia de una señal con diferentes anchos, que van desde resonancias con un ancho similar a la resolución de reconstrucción de \myj de \(\sim 2\%\) hasta resonancias amplias con un ancho de hasta el \(15\%\).
Los modelos teóricos estudiados dependen de uno o más parámetros, como por ejemplo, la masa del \ac{EQ} y su acoplamiento a los campos del \ac{SM}. De esta forma, se construye una grilla multidimensional de parámetros, donde cada punto de la grilla representa un conjunto de valores de parámetros únicos que definen el modelo de señal. En el caso de que no se encuentre un exceso significativo sobre el fondo del \ac{SM}, se establecen límites de exclusión a los valores de la grilla, excluyendo un subconjunto de posibles parámetros de la teoría.


El mayor reto en cualquier búsqueda de nueva física es la determinación precisa del fondo del \ac{SM}. Para esta búsqueda, el fondo dominante es la producción de fotones prompt. Este proceso, como se discutió en la \Sect{\ref{subsec:theory:sm:prompt_photon}}, tiene contribuciones de dos procesos, fotones directos y de fragmentación, en el que este último puede ser altamente reducido. El segundo mayor fondo surge de eventos \ac{QCD} multijet en los que uno de los jets es rico en energía \ac{EM} y se identifica erróneamente como un fotón, que incluye fundamentalmente eventos en los que un jet se fragmenta en un mesón energético \pizero, decayendo en dos fotones superpuestos que se reconstruyen como un único fotón en el detector. Finalmente, el tercer fondo, cuya contribución es despreciable, surge de procesos \ac{EW} vía la producción de \WZjet, con el bosón pesado decayendo leptónicamente y un electrón imitando a un fotón.


La estrategia más utilizada en las búsquedas de resonancias es el modelado del fondo a partir de una función analítica. Esta función suele ser un caso particular de una familia de funciones y su selección se basa en estadísticos de prueba, siendo éste uno de los pasos más relevantes del análisis.
Aunque el fondo se estima directamente a partir de los datos, se necesita simulaciones \ac{MC} del fondo para realizar optimizaciones en la selección de eventos y llevar a cabo los diferentes tests para seleccionar la función que mejor lo describe.

Para seleccionar la función de fondo, se realizan dos tipos de ajustes: ajustes de \acf{BO} y ajustes de \acf{SB}. Los ajustes \ac{BO} se caracterizan por no incluir ninguna señal y tienen como objetivo determinar la función del fondo.
Por otro lado, en los ajustes \ac{SB} se deja variar a la intensidad de señal (término introducido a continuación) y el número total de eventos de señal se utiliza para realizar los tests estadísticos a fin de determinar la óptima función que describa al fondo, disminuyendo el sesgo sobre la señal. En la \Sect{\ref{sec:bkg:modeling}}, se utilizan estos dos tipos de ajustes utilizando la simulación \ac{MC} para determinar las diferentes estrategias de ajuste que se utilizarán con los datos reales.

Para realizar la búsqueda propiamente dicha, como primer paso se lleva a cabo un ajuste \ac{BO} y se evalúan diferentes métricas para determinar si se observa un exceso significativo sobre el \ac{SM}. En caso de que no lo haya, se realizan ajustes \ac{SB} para cada modelo de señal, con el fin de establecer límites de exclusión en los parámetros de las señales. Esto se muestra en el \Ch{\ref{ch:results}}.






\section{Tratamiento estadístico}
\label{sec:strategy:stat_treatment}




\subsection{Modelo estadístico}
\label{subsec:strategy:stat_treatment:stat_model}

Los ajustes de los espectros de \myj se realizan por separado para cada región de señal del análisis (las cuales se discutirán más adelante), utilizando un método de maximum likelihood a partir de considerar un experimento de conteo en cada bin utilizando probabilidades poissonianas. El likelihood queda expresado como:
\begin{equation}
    \mathcal{L} = 
    \prod_{i=1}^{N_{\text{bins}}} \text{Pois}\left(N_i | \mu s_i + b_i\right) = 
    \prod_{i=1}^{N_{\text{bins}}} \frac{\left( \mu s_i + b_i \right)^{N_i}}{N_i !} e^{-\left( \mu s_i + b_i \right)},
\end{equation}
donde \(N_i\) es el número de eventos de datos medidos en el bin \(i\), y \(s_i\) y \(b_i\) son los números de eventos esperados de señal y fondo, estimados a partir de la simulación \ac{MC} y el ajuste funcional, respectivamente. El parámetro \(\mu\) se denomina intensidad de la señal y es un factor de escala común en todos los bines. Uno de los objetivos del ajuste es la determinación de este parámetro, que en el caso de este análisis es el \ac{POI}.

En el caso de un fondo parametrizado, \(b_i\) podría reescribirse como \(b_i = f_b\left(x, \mathbf{\theta}\right)\), donde \(x\) representa el observable (\myj en el caso de este análisis), \(f_b\) es la función de parametrización del fondo, que depende de los parámetros \(\mathbf{\theta}\) que describen la forma de la función. La función de likelihood queda entonces
\begin{equation}
    \label{eq:strategy:stat_treatment:stat_model:likelihood}
    \mathcal{L} = 
    \prod_{i=1}^{N_{\text{bins}}} \frac{\left( \mu s_i + f_b \left(x, \mathbf{\theta}\right) \right)^{N_i}}{N_i !} e^{-\left( \mu s_i + f_b \left(x, \vec{\theta}\right) \right)}.
\end{equation}

El número de eventos de señal en el bin \(i\) viene dado por el producto de la \ac{PDF2} de señal \(f_s^i(x)\), la sección eficaz por \textit{branching ratio} \(\sigma_s \times \text{Br}\), la luminosidad total integrada \(L\) y la aceptancia de la señal por la eficiencia \(A \times \varepsilon\). Entonces, el término de señal puede reescribirse como
\begin{equation}
    \label{eq:strategy:stat_treatment:stat_model:mu_si}
    \mu s_i = \left(\sigma_S \times \text{Br} \right) \times L \times \left(A \times \varepsilon\right) \times f_s^i(x).
\end{equation}
El caso especial cuando \(\mu = 0\) corresponde a una hipótesis de sólo fondo.



\subsection{Integración de incertezas sistemáticas}
\label{subsec:strategy:stat_treatment:systs}

Las incertezas sistemáticas se parametrizan como un conjunto de parámetros nuisance (\acused{NP}\acp{NP}) \(\mathbf{\theta}\) que modifican el número total de eventos esperados de señal y fondo, es decir, \(\{s_i, b_i\} \to \{s_i(\mathbf{\theta}), b_i(\mathbf{\theta})\}\). Se implementan agregando un término al likelihood:
\begin{equation}
    \prod_{k} G_k(0 | \theta_k, 1),
\end{equation}
donde \(G_k\) son las \acp{PDF2} de los \acp{NP}, y \(k\) abarca todas las variaciones sistemáticas. Además, el parámetro correspondiente se multiplica por un factor
\begin{equation}
    1 + \theta_k \delta_k
\end{equation}
para el caso de funciones de respuesta gaussianas, y
\begin{equation}
    \exp\left( \theta_k \ln\left(1 + \delta_k\right) \right) = \left(1 + \delta_k\right)^{\theta_k}
\end{equation}
para las funciones de respuesta log-normal.

Dado que la función de fondo se selecciona de forma arbritraria, pero sometiéndola a diferentes pruebas estadísticas, se añade una incerteza sistemática adicional para tener esto en cuenta, denominada \acf{SSig}. La \ac{SSig} se calcula en la \Sect{\ref{subsubsec:bkg:modeling:sigbkg:sstest}}, y se implementa en el modelo estadístico añadiendo un término más al número total de eventos:
\begin{equation}
    \mu s_i + f_b \to \mu s_i + \sigma_{\text{spur}, i} \theta_{\text{spur}} + f_b.
\end{equation}
\(\sigma_{\text{spur}}\) representa el número de eventos espurios en el bin \(i\) de la distribución que sigue la misma \ac{PDF2} que la señal, y \(\theta_{\text{spur}}\) es el \ac{NP} asociado a esta otra fuente de señal.

\subsection{Ajustes simultáneos}
\label{subsec:strategy:stat_treatment:simult_fits}

Es útil discutir en este punto el procedimiento para hacer ajustes simultáneos en diferentes distribuciones de \myj. Estos ajustes simultáneos prueban el mismo modelo de señal con la misma intensidad de señal \(\mu\) en diferentes regiones. El likelihood total es entonces el producto de los likelihood individuales dados por la \Eqn{\ref{eq:strategy:stat_treatment:stat_model:likelihood}}, y se escribe como:
\begin{align}
    \mathcal{L}_{\text{total}} =& \prod_{c\in \text{categories}} \mathcal{L}_c\\
    =& \prod_{c\in \text{categories}} \left[
        \prod_{i=1}^{N^c_{\text{bins}}} \frac{\left( n_{\text{sig}} + b_{i, c} \right)^{N_{i, c}}}{N_{i, c} !} e^{-\left( n_{\text{sig}} + b_{i, c} \right)}
    \right]
\end{align}

La implementación de las incertezas sistemáticas sigue la misma metodología que la explicada anteriormente. Además, los sistemáticos experimentales que sólo afectan a la señal están correlacionadas al 100\% entre las regiones de señal. Por otro lado, cada región de señal tiene su propia incerteza de \ac{SS}, ya que está asociada a una función de fondo y un rango de ajuste diferentes\footnote{Como se discutirá en el \Ch{\ref{ch:bkg}}, hay diferentes regiones de señal en el análisis y para cada una se selecciona una forma funcional diferente.}.








\subsection{Tests de hipótesis}
\label{subsec:strategy:stat_treatment:hypo_test}

El objetivo de una búsqueda como la presentada en esta tesis es poder especificar cuán buena es la concordancia entre los datos observados y una hipótesis dada, que típicamente es la hipótesis \enquote{nula}, \ac{BO}, o \enquote{0-señal} (\(H_0\)). Se podría probar la consistencia de los datos con cualquier hipótesis, pero se suele elegir \(H_0\) porque normalmente se puede afirmar un descubrimiento estableciendo que los datos son inconsistentes con la teoría \enquote{estándar}. Una vez establecida la inconsistencia con \(H_0\), se pueden probar varias hipótesis alternativas de señal para caracterizar el descubrimiento, denotadas \(H_1\). Para distinguir entre estas teorías se utiliza el parámetro \(\mu\), que es 0 para \(H_0\) y 1 para el caso de la señal nominal, indicando la hipótesis \(H_1\).

Al comparar los datos con las hipótesis, sus diferencias se pueden cuantificar mediante un \enquote{estadístico de prueba} que son funciones que dependen de los datos. Por lo tanto, para cada estadístico de prueba hay una \ac{PDF2} asociada.

Para probar un valor hipotético de \(\mu\), se considera el siguiente cociente:
\begin{equation}
    \label{eq:strategy:stat_treatment:hypo_test:lambdamu}
    \lambda(\mu) = \frac{
        \mathcal{L} \left(\mu, \hat{\hat{\bm{\theta}}}\right)
    }{
        \mathcal{L} \left(\hat{\mu}, \hat{\bm{\theta}}\right)
    }.
\end{equation}
El numerador de esta relación se denomina como la función \textit{profile likelihood}. La cantidad \(\hat{\hat{\bm{\theta}}}\) denota el valor de \(\bm{\theta}\) que maximiza \(\mathcal{L}\) para el \(\mu\) especificado. El denominador es la función de máximo likelihood, es decir, \(\hat{\mu}\) y \(\hat{\bm{\theta}}\) son sus estimadores de likelihood máximo.
De la \Eqn{\ref{eq:strategy:stat_treatment:hypo_test:lambdamu}}, es posible ver que \(0 \leq \lambda \leq 1\), y cuando \(\lambda \ra 1\), esto implica un buen acuerdo entre los datos y el valor hipotético de \(\mu\).
Es conveniente definir el estadístico de prueba \qmu como
\begin{equation}
    \label{eq:strategy:stat_treatment:hypo_test:qmu}
    \qmu  = - 2 \ln \lambda(\mu),
\end{equation}
de forma que un buen acuerdo se ve cuando \(\qmu \ra 0\), mientras que valores altos de \(\qmu\) indican desacuerdo entre los datos y el valor hipotético de \(\mu\).


Para medir la discrepancia entre los datos y la hipótesis \(H\), es decir, la probabilidad bajo la suposición de que \(H\) es cierta, de observar datos de igual o menor compatibilidad con la predicción de \(H\) respecto a los datos observados, se define mediante el \pval \(p_{\mu}\):
\begin{equation}
    p_{\mu} = P \left( \qmu > q_{\mu, \text{obs}} \right) = \int_{q_{\mu, \text{obs}}}^{\infty} f \left(\qmu | H\right) \dd{\qmu},
\end{equation}
donde \(q_{\mu, \text{obs}}\) es el valor del estadístico de prueba observado a partir de los datos, y \(f\left(\qmu | H\right)\) es la \ac{PDF2} de \qmu bajo la hipótesis \(H\). Un \pval pequeño significa que la hipótesis no concuerda con los datos y, por tanto, se excluye \(H\) cuando el \pval es inferior a un valor definido \(\alpha\).
Es habitual convertir el \pval en una significancia \(Z\), definida de tal forma que una variable con distribución normal tenga una probabilidad igual a ese \pval de ser encontrada a más de \(Z\) desviaciones estándar por encima de su media:
\begin{equation}
    Z = \Phi^{-1} \left(1 - p_{\mu}\right).
\end{equation}

En el contexto de la búsqueda de nueva física, los datos se contrastan con la hipótesis \(H_0\), ya que un rechazo de \(H_0\) puede significar el descubrimiento de una nueva señal. Para ello, el estadístico de prueba adopta la forma:
\begin{equation}
    q_0 = 
    \begin{cases}
        0 &\qif \hat{\mu} < 0,\\
        -2 \ln \lambda(0) &\qif \hat{\mu} \geq 0.
    \end{cases}
\end{equation}
Si los datos observados resultan ser menores que las predicciones de fondo, se tiene \(\hat{\mu} < 0\). Esto podría ser una prueba en contra de la hipótesis \ac{BO}, pero en realidad no demuestra que los datos estén compuestos por eventos señal. Con esta definición entonces, la posibilidad de descartar la hipótesis \ac{BO} se da sólo cuando \(\hat{\mu} \geq 0\), y en caso contrario \(q_0 = 0\). El \pval para este estadístico de prueba es entonces:
\begin{equation}
    \pzero = P \left( q_0 > q_{0, \text{obs}} \right) = \int_{q_{0, \text{obs}}}^{\infty} f \left(q_0 | 0 \right) \dd{q_0}.
\end{equation}
La comunidad de física de partículas define un rechazo de la hipótesis \ac{BO} con una significancia superior a \(5\sigma\) (\(p = 2.86 \times 10^{-7}\)) como el nivel apropiado para constituir un descubrimiento. Hay que subrayar que, en un contexto científico real, rechazar la hipótesis \ac{BO} en un sentido estadístico es sólo una parte del descubrimiento de un nuevo fenómeno. El grado de convencimiento de que existe un nuevo proceso dependerá en general de otros factores, como el likelihood de la hipótesis de la nueva señal y el grado en que puede describir los datos.



Cuando el \pval obtenido es superior al límite definido para un descubrimiento, no es posible rechazar la hipótesis \ac{BO}, y en ese caso se desea establecer límites al modelo probado. Para ello, se busca en cambio rechazar la hipótesis \ac{SB}, y encontrar el valor superior de \(\mu\) para el que dicho rechazo no es posible (límite superior). Se define entonces un nuevo estadístico de prueba, en el que se intercambian los papeles de las hipótesis \ac{BO} y \ac{SB}:
\begin{equation}
    \tilde{\qmu} =
    \begin{cases}
        -2 \ln \tilde{\lambda}(\mu) & \qif \hat{\mu} \leq \mu\\
        0                           & \qif \hat{\mu} > \mu
    \end{cases}
    =
    \begin{cases}
        \displaystyle -2 \ln \left(\frac{
            \mathcal{L} \left(\mu, \hat{\hat{\bm{\theta}}} \left(\mu\right)\right)
        }{
            \mathcal{L} \left(0, \hat{\hat{\bm{\theta}}} \left(0\right)\right)
        }\right) & \qif \hat{\mu} \leq 0\\
        \displaystyle -2 \ln \left(\frac{
            \mathcal{L} \left(\mu, \hat{\hat{\bm{\theta}}} \left(\mu\right)\right)
        }{
            \mathcal{L} \left(\hat{\mu}, \hat{\bm{\theta}}\right)
        }\right) & \qif 0 \leq \hat{\mu} \leq \mu\\
        0 & \qif \hat{\mu} \geq \mu\\
    \end{cases}.
\end{equation}
La razón de poner \(\qmu = 0\) cuando \(\hat{\mu} > \mu\) es que, al establecer un límite superior, no se considera que los datos con \(\hat{\mu} > \mu\) representen una menor compatibilidad con \(\mu\) que los datos obtenidos, y por lo tanto no se toman como parte de la región de rechazo de la prueba. Es decir, el límite superior se obtiene contrastando \(\mu\) con la hipótesis alternativa consistente con valores inferiores de \(\mu\). De la definición del estadístico de prueba se desprende que los valores más altos de \qmu representan una mayor incompatibilidad entre los datos y el valor hipotético de \(\mu\). Debe tenerse en cuenta que \(q_0\) no es simplemente un caso especial de \qmu con \(\mu = 0\), sino que tiene una definición diferente. Esto es, \(q_0\) es cero si los datos fluctúan para abajo (\(\hat{\mu} < 0\)), pero \qmu es cero si los datos fluctúan hacia arriba (\(\hat{\mu} > \mu\)).

Con este estadístico de prueba se pretende encontrar el \(\mu\) más alto en el que la hipótesis \ac{SB} ya no es compatible con los datos. Para ello, se define el siguiente \ac{CL}:
\begin{equation}
    \text{CL}_s = \frac{p_\mu}{1 - p_b} \equiv \frac{\text{CL}_{s+b}}{\text{CL}_b},
\end{equation}
donde
\begin{align}
    p_{\mu} &= \int_{q_{\mu, \text{obs}}}^{\infty} f \left(\qmu | \mu\right) \dd{\qmu} \equiv \text{CL}_{s+b}\\
    1 - p_{b} &= \int_{q_{\mu, \text{obs}}}^{\infty} f \left(\qmu | 0\right) \dd{\qmu} \equiv \text{CL}_{b}
\end{align}
siendo \(f \left(\qmu | \mu\right)\) la \ac{PDF2} del estadístico de prueba \qmu y \(f \left(\qmu | 0\right)\) la \ac{PDF2} bajo la hipótesis de \ac{BO}. Para valores más pequeños de \(\text{CL}_s\), hay menor compatibilidad entre los datos y la hipótesis \ac{SB}. El límite superior de \(\mu\), \(\mu_{\text{up}}\), se define como el \(\mu\) cuando \(\text{CL}_s = 0.05\), y los modelos con \(\mu > \mu_{\text{up}}\) se rechazan a un \ac{CL} del \(95\%\).




\subsection{El algoritmo de \bh}
\label{subsec:strategy:stat_treatment:bh}


El algoritmo de \bh~\cite{BumpHunter,pyBumpHunter} constituye un hipertest que combina el resultado de muchas pruebas de hipótesis individuales en un único estadístico de prueba. Recorre todas las ventanas posibles de bines adyacentes en el espectro donde se busca el bump, empezando con ventanas de un bin y aumentando su ancho hasta un umbral superior configurable (normalmente la mitad del rango total del ajuste). En cada una de estas ventanas, el número de eventos observados \(N_d\) y el número de eventos de fondo estimados \(N_b\) se suman como si la ventana fuera un único bin:
\begin{align}
    N_d &= \sum_{ \substack{ \text{bins \(i\)} \\ \text{in window} } } N_d^i\\
    N_b &= \sum_{ \substack{ \text{bins \(i\)} \\ \text{in window} } } N_b^i
\end{align}
A cada una de estas ventanas se le asigna un \pval local basado en la probabilidad poissoniana de observar al menos tantos eventos como los observados en los datos:
\begin{equation}
    p_{\text{local}} \left(N_d, N_b\right) = 
    \begin{cases}
        \Gamma \left(N_d, N_b\right)    &\qif   N_d > N_b,\\
        1                               &\qif   N_d \leq N_b,
    \end{cases}
\end{equation}
donde:
\begin{equation}
    \Gamma \left(N_d, N_b\right) = \sum_{k=N}^{\infty} \frac{N_b^k}{N_d!} e^{-N_b}
\end{equation}
es la función gamma incompleta inferior.

Dado que se testean excesos en muchas ventanas estadísticamente independientes, debe tenerse en cuenta el \textit{Look Elsewhere effect}~\cite{LookElsewhereEffect}. Este efecto describe que si se realizan, por ejemplo, 100 pruebas independientes, en promedio, en una de ellas encontrará un \pval inferior a 0.01 debido únicamente a las fluctuaciones estadísticas. Por lo tanto, el \pval local para un exceso en una ventana específica debe traducirse en un \pval global para encontrar un exceso en cualquiera de las ventanas. Para ello, el algoritmo \bh define el estadístico de prueba:
\begin{equation}
    \label{eq:strategy:stat_treatment:bh:bh_statistic}
    t = \min_{\text{windows}} \left( - \log p_{\text{local}} \right)
\end{equation}
que identifica el exceso más significativo de todas las ventanas consideradas. La distribución esperada de \(t\) se determina numéricamente con gran precisión extrayendo 2000 distribuciones de pseudo-datos (o toys) que fluctuan de manera poissoniana con respecto al fondo esperado y calculando \(t\) para cada uno de estos toys. El \pval global viene dado por la probabilidad de que una distribución de toys presente un exceso más significativo que los datos. Esto corresponde a la fracción de toys para los que \(t\) supera el valor observado \(t_{\text{obs}}\):
\begin{equation}
    \label{eq:strategy:stat_treatment:bh:bh_pval}
    p(\text{BH}) = \frac{\text{\# toys con } t > t_{\text{obs}}}{\text{\# toys}}.
\end{equation}
Debe tenerse en cuenta que, con esta definición, el \pval global puede ser superior a 0.5, produciendo una significancia global negativa. En este caso, este valor negativo no debe interpretarse como un déficit en los datos, sino como una desviación menos significativa que la desviación media observada en las distribuciones de los toys de \ac{BO}.

De este modo, \(p(\text{BH})\) sigue por definición una distribución uniforme entre 0 y 1 si los excesos sobre la estimación de fondo se deben únicamente a fluctuaciones estadísticas. Notar, por ejemplo, que \(p(\text{BH}) < 0.05\) significa entonces que menos del 5\% de los toys similares al fondo presentan un exceso mayor que el más significativo en los datos. Esta observación puede interpretarse como una prueba de la existencia de una resonancia que provoca una desviación de los datos respecto a lo esperado por el fondo.